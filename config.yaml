# Base ViT config using https://arxiv.org/pdf/2112.13492.pdf

vit_params: &vit_params
  hidden_size: 192
  num_hidden_layers: 9
  num_attention_heads: 12
  intermediate_size: 384
  hidden_act: 'relu'
  image_size: 32
  patch_size: 4
  num_channels: 3
  num_labels: 10
  device: 'cuda'

# Model configs

models: 
  vit_base:
    <<: *vit_params
    experiment_name: 'vit_base'

  vit_masked_4:
    <<: *vit_params
    k_value: 4
    experiment_name: 'vit_masked_4'

  vit_masked_8:
    <<: *vit_params
    k_value: 8
    experiment_name: 'vit_masked_8'

  vit_masked_16:
    <<: *vit_params
    k_value: 16
    experiment_name: 'vit_masked_16'

  vit_masked_32:
    <<: *vit_params
    k_value: 32
    experiment_name: 'vit_masked_32'

  vit_masked_64:
    <<: *vit_params
    k_value: 64
    experiment_name: 'vit_masked_64'

  vit_masked_128:
    <<: *vit_params
    k_value: 128
    experiment_name: 'vit_masked_128'

  vit_masked_256:
    <<: *vit_params
    k_value: 256
    experiment_name: 'vit_masked_256'

# Training config

dataset_path: "./data"
save_path: "./models"
lr: 0.003
num_epochs: 400
weight_decay: 0.05
test_interval: 5
periodic_save_interval: 40
device: 'cuda'

# Dataloader config

train_batch_size: 128
test_batch_size: 512
num_workers: 4

# Scheduler configs

cos_annealing:
  first_cycle_steps: 100
  cycle_mult: 1
  max_lr: 0.003
  min_lr: 1e-6
  warmup_steps: 10
  gamma: 0.9

multistep_lr:
  milestones: [100, 150, 200, 250, 300]
  gamma: 0.15

# Adversarial attacks

epsilons: [0, .05, .1, .15, .20, .25, .3, .35]
cifar_mean: [0.4914, 0.4822, 0.4465]
cifar_std: [0.2023, 0.1994, 0.2010]
adv_batch_size: 1
